{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T06:17:02.403706Z",
     "start_time": "2021-03-20T06:16:58.195841Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegressionCV as logreg\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from transformers import pipeline\n",
    "\n",
    "pd.set_option('display.max_colwidth', 255)\n",
    "plt.style.use(\"dark_background\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T06:17:07.501480Z",
     "start_time": "2021-03-20T06:17:02.404679Z"
    }
   },
   "outputs": [],
   "source": [
    "poli_pos = pd.read_csv('./data/poli_pos.csv')\n",
    "poli_neg = pd.read_csv('./data/poli_neg.csv')\n",
    "poli_val = pd.read_csv('./data/poli_val.csv')\n",
    "topics_neg = pd.read_csv('./data/topics_neg.csv')\n",
    "topics_pos = pd.read_csv('./data/topics_pos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T06:17:07.511240Z",
     "start_time": "2021-03-20T06:17:07.502465Z"
    }
   },
   "outputs": [],
   "source": [
    "poli_neg['target'] = 0 \n",
    "poli_pos['target'] = 1 \n",
    "topics_neg['target'] = 0\n",
    "topics_pos['target'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T06:17:07.521979Z",
     "start_time": "2021-03-20T06:17:07.514171Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'datetime', 'tweet_id', 'text', 'user_name', 'target'], dtype='object')\n",
      "Index(['Unnamed: 0', 'datetime', 'tweet_id', 'text', 'user_name', 'target'], dtype='object')\n",
      "Index(['Unnamed: 0', 'datetime', 'tweet_id', 'text', 'user_name', 'target'], dtype='object')\n",
      "Index(['Unnamed: 0', 'datetime', 'tweet_id', 'text', 'keyword', 'target'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(poli_neg.columns)\n",
    "print(poli_pos.columns)\n",
    "print(topics_neg.columns)\n",
    "print(topics_pos.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T06:17:07.681072Z",
     "start_time": "2021-03-20T06:17:07.523930Z"
    }
   },
   "outputs": [],
   "source": [
    "# note: I will have to exclude the keyword column until I have added appropriate\n",
    "# keywords throughout the data.  Right now there are too few, and they are too predictive.\n",
    "poli_neg.drop(columns=['Unnamed: 0', 'tweet_id', 'user_name', 'datetime'], inplace=True)\n",
    "poli_pos.drop(columns=['Unnamed: 0', 'tweet_id', 'user_name', 'datetime'], inplace=True)\n",
    "topics_neg.drop(columns=['Unnamed: 0', 'tweet_id', 'user_name', 'datetime'], inplace=True)\n",
    "topics_pos.drop(columns=['Unnamed: 0', 'tweet_id', 'datetime', 'keyword'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T06:17:07.726936Z",
     "start_time": "2021-03-20T06:17:07.682042Z"
    }
   },
   "outputs": [],
   "source": [
    "df_topics = pd.concat([topics_pos, topics_neg], ignore_index=True)\n",
    "df_poli = pd.concat([poli_pos, poli_neg], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T06:17:07.794281Z",
     "start_time": "2021-03-20T06:17:07.727913Z"
    }
   },
   "outputs": [],
   "source": [
    "X_poli = df_poli.drop(columns=['target'])\n",
    "y_poli = df_poli['target']\n",
    "X_topics = df_topics.drop(columns=['target'])\n",
    "y_topics = df_topics['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T06:17:09.082464Z",
     "start_time": "2021-03-20T06:17:07.796235Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the data into the training and testing sets.\n",
    "X_train_poli, X_test_poli, y_train_poli, y_test_poli = train_test_split(X_poli,\n",
    "                                                                        y_poli,\n",
    "                                                                        test_size=0.2,\n",
    "                                                                        stratify=y_poli,\n",
    "                                                                        random_state=42)\n",
    "\n",
    "X_train_topics, X_test_topics, y_train_topics, y_test_topics = train_test_split(X_topics,\n",
    "                                                                                y_topics,\n",
    "                                                                                test_size=0.2,\n",
    "                                                                                stratify=y_topics,\n",
    "                                                                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T06:17:09.091218Z",
     "start_time": "2021-03-20T06:17:09.084387Z"
    }
   },
   "outputs": [],
   "source": [
    "# custom stopwords to manually zero in on more predictive words\n",
    "\n",
    "custom = ['https', 'trump', 'realdonaldtrump', 'rt', 'health', 'country', \n",
    "          'just', 'help', 'thank', 'time', 'senate', 'http', 'american', \n",
    "          'americans', 'act', 'vote', 'pandemic', 'like', 'america', 'state', \n",
    "          'support', 'day', 'workers', 'right', 'years', 'working', 'good', \n",
    "          'thanks', 'families', 'covid', 'crisis', 'election', 'big', \n",
    "          'congress', 'let', 'want', 'national', 'going', 'know', 'relief',  \n",
    "          'house', 'public', 'year', 'federal', 'continue', 've', \n",
    "          'states', 'justice', 'way', 'ensure', 'jobs', 'law', 'businesses', \n",
    "          'proud', 'administration', 'small', 'world', 'stop', 'job', 'safe',\n",
    "          'best', 'important', 'biden', 'doing', 'economy', 'better', 'needs',\n",
    "          'week', 'join', 'funding', 'forward', 'economic', 'amp', 'did', \n",
    "          'millions', 'need', 'today', 'work', 'new']\n",
    "\n",
    "\n",
    "combined_words = text.ENGLISH_STOP_WORDS.union(custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T06:17:09.100001Z",
     "start_time": "2021-03-20T06:17:09.093169Z"
    }
   },
   "outputs": [],
   "source": [
    "# implement countvectorizer\n",
    "cvec = CountVectorizer(analyzer=\"word\",\n",
    "                       encoding='utf-8',\n",
    "                       decode_error='ignore',\n",
    "                       strip_accents='unicode',\n",
    "                       lowercase=True,\n",
    "                       max_df=0.95,\n",
    "                       min_df=0.01,\n",
    "                       stop_words=combined_words,\n",
    "                       max_features=5000,\n",
    "                       ngram_range=(1, 3))\n",
    "\n",
    "tvec = TfidfVectorizer(encoding='utf-8',\n",
    "                       decode_error='ignore',\n",
    "                       strip_accents='unicode',\n",
    "                       lowercase=True,\n",
    "                       analyzer='word',\n",
    "                       stop_words=combined_words,\n",
    "                       ngram_range=(1, 3),\n",
    "                       max_df=.8,\n",
    "                       min_df=.05,\n",
    "                       max_features=10000,\n",
    "                       norm='l2')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T06:17:09.352785Z",
     "start_time": "2021-03-20T06:17:09.100978Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_topics.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T06:17:09.462096Z",
     "start_time": "2021-03-20T06:17:09.353762Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-294d3c14820a>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_topics.dropna(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "X_test_topics.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T06:17:11.168628Z",
     "start_time": "2021-03-20T06:17:09.463073Z"
    }
   },
   "outputs": [],
   "source": [
    "for entry in X_train_poli['text']:\n",
    "    entry = lemmatizer.lemmatize(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T06:17:11.256935Z",
     "start_time": "2021-03-20T06:17:11.169564Z"
    }
   },
   "outputs": [],
   "source": [
    "for entry in X_test_poli['text']:\n",
    "    entry = lemmatizer.lemmatize(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T06:17:16.437575Z",
     "start_time": "2021-03-20T06:17:11.258885Z"
    }
   },
   "outputs": [],
   "source": [
    "for entry in X_train_topics['text']:\n",
    "    entry = lemmatizer.lemmatize(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T06:17:17.712884Z",
     "start_time": "2021-03-20T06:17:16.438565Z"
    }
   },
   "outputs": [],
   "source": [
    "for entry in X_test_topics['text']:\n",
    "    entry = lemmatizer.lemmatize(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T06:17:29.771590Z",
     "start_time": "2021-03-20T06:17:17.714832Z"
    }
   },
   "outputs": [],
   "source": [
    "# apply cvec to train and test sets\n",
    "poli_X = pd.DataFrame(cvec.fit_transform(X_train_poli['text']).todense(),\n",
    "                      columns=cvec.get_feature_names())\n",
    "\n",
    "poli_X_test = pd.DataFrame(cvec.transform(X_test_poli['text']).todense(),\n",
    "                           columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T06:19:51.813201Z",
     "start_time": "2021-03-20T06:17:29.772561Z"
    }
   },
   "outputs": [],
   "source": [
    "topics_X = pd.DataFrame(cvec.fit_transform(X_train_topics['text']).todense(),\n",
    "                      columns=cvec.get_feature_names())\n",
    "\n",
    "topics_X_test = pd.DataFrame(cvec.transform(X_test_topics['text']).todense(),\n",
    "                           columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T06:19:51.817101Z",
     "start_time": "2021-03-20T06:19:51.814173Z"
    }
   },
   "outputs": [],
   "source": [
    "# # re-add the username and datetime columns to the temporary df, then replace the original with the vectorized version\n",
    "\n",
    "# X_train_poli = pd.concat([poli_X, X_train_poli['datetime']], axis=1)\n",
    "# X_test_poli = pd.concat([poli_X_test, X_test_poli['datetime']], axis=1)\n",
    "# X_train_topics = pd.concat([topics_X, X_train_topics['datetime']], axis=1)\n",
    "# X_test_topics = pd.concat([topics_X_test, X_test_topics['datetime']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T06:19:51.832719Z",
     "start_time": "2021-03-20T06:19:51.826862Z"
    }
   },
   "outputs": [],
   "source": [
    "l1 = logreg(cv=5,\n",
    "            penalty='l1',\n",
    "            scoring=None,\n",
    "            solver='liblinear',\n",
    "            tol=0.001,\n",
    "            max_iter=200,\n",
    "            class_weight='balanced',\n",
    "            n_jobs=-2,\n",
    "            verbose=1,\n",
    "            refit=True,\n",
    "            intercept_scaling=1.0,\n",
    "            multi_class='auto',\n",
    "            random_state=42,\n",
    "            l1_ratios=None,)\n",
    "\n",
    "l2 = logreg(cv=5,\n",
    "            penalty='l2',\n",
    "            scoring=None,\n",
    "            solver='lbfgs',\n",
    "            tol=0.001,\n",
    "            max_iter=200,\n",
    "            class_weight='balanced',\n",
    "            n_jobs=-2,\n",
    "            verbose=1,\n",
    "            refit=True,\n",
    "            intercept_scaling=1.0,\n",
    "            multi_class='auto',\n",
    "            random_state=42,\n",
    "            l1_ratios=None,)\n",
    "\n",
    "lr_net = logreg(cv=5,\n",
    "                penalty='elasticnet',\n",
    "                scoring=None,\n",
    "                solver='saga',\n",
    "                tol=0.001,\n",
    "                max_iter=200,\n",
    "                class_weight='balanced',\n",
    "                n_jobs=-2,\n",
    "                verbose=1,\n",
    "                refit=True,\n",
    "                intercept_scaling=1.0,\n",
    "                multi_class='auto',\n",
    "                random_state=42,\n",
    "                l1_ratios=.5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T06:19:51.842478Z",
     "start_time": "2021-03-20T06:19:51.834670Z"
    }
   },
   "outputs": [],
   "source": [
    "topics_sc1 = StandardScaler()\n",
    "\n",
    "topics_pipe1 = Pipeline([\n",
    "    ('scaler', topics_sc1),\n",
    "    ('model', l1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establishing null model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T06:19:51.864926Z",
     "start_time": "2021-03-20T06:19:51.843454Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.97663\n",
       "1    0.02337\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_topics.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T06:19:51.874729Z",
     "start_time": "2021-03-20T06:19:51.865969Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.976629\n",
       "1    0.023371\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_topics.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T06:21:10.886625Z",
     "start_time": "2021-03-20T06:19:51.875705Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 11 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   2 out of   5 | elapsed:  1.2min remaining:  1.8min\n",
      "[Parallel(n_jobs=-2)]: Done   5 out of   5 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [368616, 368615]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-0113a0cbfdfd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtopics_train1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtopics_pipe1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopics_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_topics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtopics_test1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtopics_pipe1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopics_X_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_topics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'LogReg L1, score on training set: {topics_train1}, score on test set: {topics_test1}.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;31m# update the docstring of the returned function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    609\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m             \u001b[0mscore_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sample_weight'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mscore_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   2084\u001b[0m         \u001b[0mscoring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_scorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2085\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2086\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[0;32m    167\u001b[0m                           \u001b[0mcategory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFutureWarning\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m                           stacklevel=2)\n\u001b[1;32m--> 169\u001b[1;33m         return self._score(partial(_cached_call, None), estimator, X, y_true,\n\u001b[0m\u001b[0;32m    170\u001b[0m                            sample_weight=sample_weight)\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\u001b[0m in \u001b[0;36m_score\u001b[1;34m(self, method_caller, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[0;32m    210\u001b[0m                                                  **self._kwargs)\n\u001b[0;32m    211\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m             return self._sign * self._score_func(y_true, y_pred,\n\u001b[0m\u001b[0;32m    213\u001b[0m                                                  **self._kwargs)\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;31m# Compute accuracy for each possible representation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'multilabel'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \"\"\"\n\u001b[1;32m---> 81\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[0;32m    256\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [368616, 368615]"
     ]
    }
   ],
   "source": [
    "topics_pipe1.fit(topics_X, y_train_topics)\n",
    "\n",
    "topics_train1 = topics_pipe1.score(topics_X, y_train_topics)\n",
    "topics_test1 = topics_pipe1.score(topics_X_test, y_test_topics)\n",
    "print(f'LogReg L1, score on training set: {topics_train1}, score on test set: {topics_test1}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T06:21:10.887601Z",
     "start_time": "2021-03-20T06:19:40.064Z"
    }
   },
   "outputs": [],
   "source": [
    "topics_X.sum().sort_values(ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T06:21:10.888577Z",
     "start_time": "2021-03-20T06:19:40.530Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot top occuring words\n",
    "topics_X.sum().sort_values(ascending=False).head(30).plot(kind='barh', figsize=(13,13));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T06:21:10.889552Z",
     "start_time": "2021-03-20T06:19:41.597Z"
    }
   },
   "outputs": [],
   "source": [
    "topics_sc2 = StandardScaler()\n",
    "\n",
    "topics_pipe2 = Pipeline([\n",
    "    ('scaler', topics_sc2),\n",
    "    ('model', l2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T06:21:10.890528Z",
     "start_time": "2021-03-20T06:19:41.809Z"
    }
   },
   "outputs": [],
   "source": [
    "topics_pipe2.fit(topics_X, y_train_topics)\n",
    "\n",
    "topics_train2 = topics_pipe2.score(topics_X, y_train_topics)\n",
    "topics_test2 = topics_pipe2.score(topics_X_test, y_test_topics)\n",
    "print(f'LogReg L2, score on training set: {topics_train2}, score on test set: {topics_test2}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T06:21:10.891504Z",
     "start_time": "2021-03-20T06:19:41.962Z"
    }
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=250,\n",
    "                            criterion='gini',\n",
    "                            max_depth=5,\n",
    "                            min_samples_split=2,\n",
    "                            min_samples_leaf=1,\n",
    "                            min_weight_fraction_leaf=0.0,\n",
    "                            max_features='auto',\n",
    "                            max_leaf_nodes=None,\n",
    "                            min_impurity_decrease=0.0,\n",
    "                            min_impurity_split=None,\n",
    "                            bootstrap=True,\n",
    "                            oob_score=False,\n",
    "                            n_jobs=None,\n",
    "                            random_state=None,\n",
    "                            verbose=0,\n",
    "                            warm_start=False,\n",
    "                            class_weight=None,\n",
    "                            ccp_alpha=0.0,\n",
    "                            max_samples=None,\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T06:21:10.892483Z",
     "start_time": "2021-03-20T06:19:42.217Z"
    }
   },
   "outputs": [],
   "source": [
    "politopics\n",
    "topics_pipe4 = Pipeline([\n",
    "    ('scaler', topics_sc1),\n",
    "    ('model', rf)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T06:21:10.893458Z",
     "start_time": "2021-03-20T06:19:42.434Z"
    }
   },
   "outputs": [],
   "source": [
    "topics_pipe4.fit(topics_X, y_train_topics)\n",
    "\n",
    "topics_train4 = topics_pipe4.score(topics_X, y_train_topics)\n",
    "topics_test4 = topics_pipe4.score(topics_X_test, y_test_topics)\n",
    "print(f'Random Forest, score on training set: {topics_train4}, score on test set: {topics_test4}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T06:21:10.894434Z",
     "start_time": "2021-03-20T06:19:42.612Z"
    }
   },
   "outputs": [],
   "source": [
    "# currently not working, unsure why\n",
    "\n",
    "# Train\n",
    "rf.fit(poli_X, y_train_poli)\n",
    "# Extract single tree\n",
    "estimator = rf.estimators_[5]\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "# Export as dot file\n",
    "export_graphviz(estimator, out_file='tree.dot', \n",
    "                feature_names = poli_X.columns,\n",
    "                class_names = y_train_poli,\n",
    "                rounded = True, proportion = False, \n",
    "                precision = 2, filled = True)\n",
    "\n",
    "# Convert to png using system command (requires Graphviz)\n",
    "from subprocess import call\n",
    "call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n",
    "\n",
    "# Display in jupyter notebook\n",
    "from IPython.display import Image\n",
    "Image(filename = 'tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T06:21:10.896386Z",
     "start_time": "2021-03-20T06:19:44.260Z"
    }
   },
   "outputs": [],
   "source": [
    "topics_sc1 = StandardScaler()\n",
    "\n",
    "topics_pipe1 = Pipeline([\n",
    "    ('scaler', topics_sc1),\n",
    "    ('model', l1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T06:21:10.896386Z",
     "start_time": "2021-03-20T06:19:44.762Z"
    }
   },
   "outputs": [],
   "source": [
    "topics_pipe1.fit(topics_X, y_train_poli)\n",
    "\n",
    "topics_train1 = topics_pipe1.score(topics_X, topics)\n",
    "topics_test1 = topics_pipe1.score(topics_X_test, topics)\n",
    "print(f'LogReg L2, score on training set: {topics_train1}, score on test set: {topics_test1}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T20:01:17.524635Z",
     "start_time": "2021-03-18T19:56:59.960Z"
    }
   },
   "outputs": [],
   "source": [
    "poli_cvec = pd.DataFrame(cvec.fit_transform(df_poli['text']).todense(), \n",
    "                         columns=cvec.get_feature_names())\n",
    "\n",
    "poli_cvec.sum().sort_values(ascending=False).head(10).plot(kind='barh');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T20:01:17.525610Z",
     "start_time": "2021-03-18T19:56:59.961Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert training data to dataframe\n",
    "poli_tiff = pd.DataFrame(tvec.fit_transform(df_poli['text']).todense(), \n",
    "                          columns=tvec.get_feature_names())\n",
    "\n",
    "# plot top occuring words\n",
    "poli_tiff.sum().sort_values(ascending=False).head(10).plot(kind='barh');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
